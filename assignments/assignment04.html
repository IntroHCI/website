<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Introduction to Human-Computer Interaction: Assignment">
    <title>Intro to HCI: Assignment 4</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/bootstrap-theme.min.css" rel="stylesheet">
    <link href="../css/hci.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>
  <body data-spy="scroll" data-target=".sidenav">

    <!--
    IntroHCI assignment template
    ------------------
    To create a new assignment:
    1) Write all the HTML as you normally would underneath the #assignment-description div.
    2) Anything that you want to appear in the navbar on the left needs to be enclosed
        in a <div id="unique-id" class="sidenav-anchor">. On page load, hci.js will look up
        anything enclosed in .sidenav-anchor, find the first header (h1-h6) inside it, and add
        it to the navigation pane on the left. Make sure it has a unique div id so that there is something
        to anchor the hyperlink to.
    3) I recommend going with Bootstrap's recommendation and putting any <table class="table"> elements enclosed in a
        <div class="table-responsive">.
    -->


    <!-- Fixed navbar -->
    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Intro to HCI</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class=""><a href="../index.html">Home</a></li>
            <li><a href="../index.html#calendar">Calendar</a></li>
            <li><a href="../lab/">Lab</a></li>
            <li class="dropdown">
                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Logistics <b class="caret"></b></a>
                <ul class="dropdown-menu">
                  <li><a href="../logistics.html#prereqs">Prerequisites</a></li>
                  <li><a href="../logistics.html#lab">Lab</a></li>
                  <li><a href="../logistics.html#grading">Grading</a></li>
                  <li><a href="../logistics.html#attendance">Attendance</a></li>
                  <li><a href="../logistics.html#cs77">CS 77</a></li>
                  <li><a href="../logistics.html#faq">FAQ</a></li>
                </ul>
            </li>
            <li><a href="http://www.coursera.org">Submit Work</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="staffemailorpiazzaforum@university.edu">Questions</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
        <div class="row">
            <div class="col-md-3">
                <div class="sidenav" data-spy="affix">
                    <ul class="nav nav-stacked" id="sidenav-content">
                    </ul>
                </div> <!-- /sidenav -->
            </div> <!-- /col-md-3 -->
            <div class="col-md-9" id="assignment-description">

                <div>
                    <h1>Assignment 4: Heuristic evaluation</h1>
                </div>

<!--
                <div style="color:red">
                    <h2 style="color:red">Important notice â€” Read carefully</h2>
                    <p>This week, you will start working on what will become your Web site. Remember, this class focuses on interaction, not implementation. We encourage you to make the coolest application you can, but don't require textual programming. If you're most comfortable with graphical tools, we recommend <a href="https://class.coursera.org/hci-004/wiki/view?page=DownloadingTools">Justinmind</a>. (You're of course welcome to use text tools if you prefer; the choice is yours.)</p>
                    <p> Now is the time to think about what design tool you'll use. Consider what interactions are most important to your application and what prototyping tools support those interactions. Think about that now so you don't get surprised when designing later. &nbsp;Please note that you will not start using this tool until the next assignment (right now, you are just prototyping in Balsamiq or an equivalent low-fidelity tool); this is just about thinking for the future.</p>
                </div> -->
                <div id="brief" class="sidenav-anchor">
                    <h2>Brief</h2>
                    <p>As Carolyn Snyder writes, "Paper prototyping is a variation of usability testing where the representative users perform realistic tasks by interacting with a paper version of the interface that is manipulated by a person 'playing computer' who doesn't explain how the interface is intended to work." In this assignment, your group will conduct heuristic evaluations (HEs) of your paper prototypes and you will individually help out other groups by evaluating theirs. This will complete the prototyping phase of the project, providing you with the feedback you need to begin implementing.</p>
                </div>

                <div id="assignment" class="sidenav-anchor">
                    <h2>Assignment</h2>
                    <p>The heuristic evaluations will be a way to highlight usability issues in your rapid electronic prototypes. Heuristic evaluations will follow the <a href="http://www.nngroup.com/articles/how-to-conduct-a-heuristic-evaluation/" target="_blank">"How to Conduct a Heuristic Evaluation"</a> and <a href="http://www.nngroup.com/articles/ten-usability-heuristics/" target="_blank">"Ten Usability Heuristics"</a> readings by Jakob Nielsen. Using Nielsen's ten heuristics, you as an evaluator will list as many usability issues as possible. It is less important that the evaluators worry about exactly which heuristic the usability issues occur in. The heuristics are there to help evaluators find a variety of different types of usability issues. For each usability issue found the evaluator should capture a description of the issue written as concisely and as specifically as possible.</p>
                </div>

                <div id="walkthrough" class="sidenav-anchor">
                    <h4>Walkthrough</h4>
                    <p>First of all, you need to master the skill of operating your paper prototype. Don't embarrass yourself in front of your expert evaluators by taking five minutes to find the next bit of the prototype to swap in. The smoother your paper prototype runs, the better. All team members need to learn the computer part, so go through a couple practice runs of each of your two prototypes. Take turns, with one of you being the evaluator and one being the computer. Practice runs like these are called walkthroughs. Walkthroughs will get you comfortable operating the paper prototype and help you identify problems with it (for example missing pieces or dead ends).</p>
                </div>

                <div id="conduct" class="sidenav-anchor">
                    <h4>Conduct a Heuristic Evaluation</h4>
                    <p>Now that you have prepared yourself to run the paper prototype smoothly, you are ready to conduct a HE session. Your whole team should be present for all sessions. To facilitate this we suggest you try as hard as you can to schedule a block of time for all of your expert evaluators to come in and perform back-to-back evaluations (one at a time).</p>
                    <p>One person from your team will be the facilitator. What's a facilitator? The facilitator should greet the evaluator, explain how the session will work, and give a brief introduction to your prototypes. One (or more, if necessary) person should be the computer. Any team member who is not currently acting as either of those will observe and take notes/pictures.</p>
                    <p>Make things easy for your evaluator by printing a sheet of Nielsen's heuristics. Prepare a sheet for them to fill out while evaluating. (Try using <a href="http://stanford.edu/class/cs147/worksheet.pdf" target="_blank">this worksheet</a>.) A good idea is to set up a spreadsheet on a laptop that you and the evaluator can share. (This spreadsheet makes it easy for the evaluator to do his assignment and for you to improve the design.) Remember that the evaluator is the expert. Let them explore and evaluate the interface as they choose, but make sure that they go over each of your two proposed prototypes.</p>
                </div>

                <div id="evaluate" class="sidenav-anchor">
                    <h4>Be an Expert Evaluator</h4>
                    <p>As an evaluator, keep in mind that you are the expert and you should go through the interface the way that you would prefer to. Be thorough and write down all problems you can find. Don't try to be "nice" by not reporting problems, everything you find will help the team improve their interface. Since you are evaluating two prototypes, try to make your feedback comparative. For example, if a prototype had a big problem that you also found in the other prototype, make a note of it. If one prototype had a problem that, for some interesting reason or another, was solved or not an issue in the other prototype, make a note of that as well. You don't have to compare both prototypes in every sentence in your evaluation (and it's even fine if most of your feedback isn't comparative), but you should highlight enough similarities and differences between the two prototypes so that the group receiving your feedback will understand the advantages and drawbacks and each design. Your feedback should help the group understand and decide which features of which designs they should implement in the coming weeks.</p>
                    <p>Use Nielsen's heuristics as a guide and specify what heuristic(s) each problem is related to. If you come across problems that don't fall neatly into particular heuristics, mark that no heuristics apply. As long as your discussion of the problems clearly shows that you understand and are trying to apply Nielsen's heuristics, you will get full credit in the heuristics category (see the grading rubric). Getting the problem written down with a severity rating is the more important part. Use Nielsen's <a href="http://www.nngroup.com/articles/how-to-rate-the-severity-of-usability-problems/ target="_blank"">Severity Ratings for Usability Problems</a>.</p>
                    <p>If you use a Google document, make sure that both you and the team you are evaluating has access to your evaluation. Go over both of the team's prototypes. You can use the comparison to inform your feedback. The evaluation will be part of your submission this week.</p>
                </div>

                <div id="meetwithothers" class="sidenav-anchor">
                    <h4>Meet with Other Evaluators</h4>
                    <p>Meet with the two other evaluators of the same prototype and (at least) one member of the prototype's design team. Discuss the general characteristics of the UI you evaluated, and suggest potential improvements to address major usability problems that you identified. Aggregate your evaluation with the other evaluations for the prototypes. Did you agree on the most severe usability problems with the other evaluators? Come to an agreement on which problems are most important, then brainstorm potential solutions with the team you evaluated.</p>
                    <p>Finally, with the other evaluators, distill all of this down to one paragraph where you address the major problems you all identified, as well as the potential solutions. Include a sentence or two reflecting on what kinds of things you found heuristic evaluation valuable for, and what kinds of things it's not very useful for. All evaluators of the same prototypes will submit the same paragraph.</p>
                </div>

                <div id="examples" class="sidenav-anchor">
                    <h2>Student Examples</h2>
                    <p>Here are two student examples from last year. Keep in mind that the assignment was slightly different last year.</p>
                    <ul>
                        <li><a href="./examples/a04example1.html">Example 1</a> - This is an example of an A+ level assignment. This student did an incredibly thorough heuristic evaluation. We especially liked how this student incorporated comparative feedback.</li>
                        <li><a href="./examples/a04example2.html">Example 2</a> - This is an example of a B level assignment. The level of feedback for the prototypes was very light, and the comparisons between prototypes were quite sparse.</li>
                    </ul>
                </div>

                <div id="submit" class="sidenav-anchor">
                    <h2>Submit</h2>
                    <p>This assignment will be submitted <b>individually</b>.</p>
                    <ul>
                        <li>A heuristic evaluation of another group's two paper prototypes. This comprises a bulleted list of usability issues you found, along with their severity, for each prototype. Include comparative feedback between the two prototypes.</li>
                        <li>A paragraph, written jointly with the other evaluators, addressing the major problems identified with the prototypes and potential solutions. Include a sentence or two reflecting on what kinds of things you found heuristic evaluation valuable for, and what kinds of things it's not very useful for. All evaluators of the same prototypes will submit this paragraph.</li>
                    </ul>
                </div>

                <div id="evaluation" class="sidenav-anchor">
                    <h2>Evaluation criteria &amp; Grading rubric</h2>
                    <div class="table-responsive">
                        <table class="table">
                            <tbody>
                                <tr>
                                    <th id="guiding" valign="top">Category</th>
                                    <th id="unsatisfactory" valign="top">Nope</th>
                                    <th id="minimum" valign="top">Weak</th>
                                    <th id="satisfactory" valign="top">Proficient</th>
                                    <th id="above" valign="top">Mastery</th>
                                </tr>
                                <tr>
                                    <td>Relation to Nielsenâ€™s Heuristics<br /><span style="color:lightGray;">3 points</span></td>
                                    <td>No relation to Nielsenâ€™s heuristics.</td>
                                    <td>Very few relations to Nielsen's heuristics at all.</td>
                                    <td>Evaluation applies heuristics to partner's prototype in a useful, organized way--most of the time. There may be a few parts, though, that don't seem as related to the heuristics, which may make the reader wonder if the evaluator was still familiarizing himself with the heuristics during the evaluation.</td>
                                    <td>Clearly grounded in Nielsen's heuristics. This evaluation could be used in the lecture about Nielsen's heuristics.</td>
                                </tr>
                                <tr>
                                    <td>Volume of Feedback<br /><span style="color:lightGray;">3 points</span></td>
                                    <td>No feedback given.</td>
                                    <td>There is so little feedback that it suggests that there are virtually no problems with the prototypes, which (based on the TAs' knowledge of the prototypes being tested) is definitely not true.</td>
                                    <td>There is a good amount of feedback, but there probably could be more. A reader gets the feeling that the evaluator was trying to be "nice" or "hold back" certain feedback.</td>
                                    <td>You really couldn't ask for more. Clearly the evaluator "showed no mercy."</td>
                                </tr>
                                <tr>
                                    <td>Quality of Feedback<br /><span style="color:lightGray;">3 points</span></td>
                                    <td>No feedback given.</td>
                                    <td>Most of the feedback was obvious (you could have come up with it without really going through the HE process) or vague (the designers might not be sure what the problem you're referring to is).</td>
                                    <td>There is a good amount of high-quality feedback, but some feedback may still be obvious or vague. It also may lack a useful comparison between the two prototypes.</td>
                                    <td>Insightful, widely varied feedback that compared the two prototypes. This feedback will give the designers a solid grasp of the advantages and drawbacks of each design, and help them decide which design or which features to implement.</td>
                                </tr>
                                <tr>
                                    <td>Severity Ratings<br /><span style="color:lightGray;">3 points</span></td>
                                    <td>No severity ratings given.</td>
                                    <td>At least half of the problems have no ratings or confusing ratings. An example of a confusing rating: a piece of text that wasn't centered was somehow rated more severe than a missing screen or broken link.</td>
                                    <td>Most of the problems have good ratings, but a few of them still have confusing ratings.</td>
                                    <td>Pretty much all of the problems have good severity ratings. As a result, the group that made the prototypes can make a good prioritized list of problems to address in their designs.</td>
                                </tr>
                                <tr>
                                    <td>Aggregate Evaluation &&nbsp;reflection<br /><span style="color:lightGray;">3 points</span></td>
                                    <td>None given.</td>
                                    <td>The conclusions reached by the evaluators as a group was just a rehash of all the individual evaluations. No potential solutions to major usability problems were brainstormed.</td>
                                    <td>The conclusions reached by the evaluators as a group was just a rehash of all the individual evaluations. Potential solutions to major usability problems were brainstormed, but they were obvious, and something that the group that made the prototypes could have figured out for themselves.</td>
                                    <td>The conclusions reached by the evaluators as a group were more insightful than the aggregation of all the individual evaluations. Potential solutions to major usability problems were creative and insightful.</td>
                                </tr>
                                <tr>
                                    <td headers="guiding" colspan=2><br>Outside the Box<br><span style="color:lightGray;"><span style="color:lightGray;">+1. Up to 5% of submissions.</span></td>
                                    <td colspan=3 align=right>Multiple parts of the feedback were innovative and interesting. The student has obviously spent a lot of time and energy applying Nielsen's Heuristics in a clever manner.</td>
                                </tr>


                            </tbody></table>
                    </div>
                </div>
                <div id="faq" class="sidenav-anchor">
                  <br><h3>Frequently Asked Questions</h3>

                  <div class="row">
                    <div class="col-md-4">
                        <strong>How many heuristic violations should we find?</strong>
                      </div>
                      <div class="col-md-8">
                        <p> We're not requiring an exact number of heuristic violations-- you can follow the assignment examples as a guideline for what we are expecting.</p>
                      </div>
                  </div>

                  <div class="row">
                    <div class="col-md-4">
                        <strong>Do we need to differentiate between the prototypes in our evaluations?</strong>
                      </div>
                      <div class="col-md-8">
                        <p> Yes, you should clearly label "Prototype 1" or "Prototype 2."</p>
                      </div>
                  </div>
                </div>
            </div>


    </div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/hci.js"></script>
  </body>
</html>
